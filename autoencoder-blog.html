<!DOCTYPE html>
 <html lang="en">
  <head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width,initial-scale=1.0"> <title>Understanding Autoencoders | Soumya Savarn</title> 
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
  <script src="https://cdn.jsdelivr.net/npm/tensorflow@2.8.0/dist/tf.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
  <style> :root { --primary-color: #232526; --accent-color: #ffb347; --bg-color: #f8f9fa; --text-color: #333; --light-text: #666; --card-bg: #fff; --code-bg: #f1f1f1; }
  body {
    font-family: 'Segoe UI', Arial, sans-serif;
    margin: 0;
    background: var(--bg-color);
    color: var(--text-color);
    line-height: 1.8;
  }
  
  .back-btn {
    display: inline-block;
    padding: 8px 16px;
    color: var(--primary-color);
    text-decoration: none;
    margin-bottom: 32px;
    transition: color 0.2s;
    font-weight: 500;
    border-radius: 4px;
    background: rgba(255, 179, 71, 0.1);
  }
  
  .back-btn:hover {
    color: var(--accent-color);
    background: rgba(255, 179, 71, 0.2);
  }
  
  .blog-post {
    max-width: 800px;
    margin: 0 auto;
  }
  
  .blog-meta {
    color: var(--light-text);
    margin: 16px 0 32px;
    font-size: 0.95em;
  }
  
  .blog-content {
    line-height: 1.8;
    font-size: 1.1em;
  }
  
  .blog-content h2 {
    margin-top: 48px;
    color: var(--primary-color);
    border-bottom: 2px solid var(--accent-color);
    padding-bottom: 8px;
    display: inline-block;
  }
  
  .blog-content h3 {
    margin-top: 32px;
    color: var(--primary-color);
  }
  
  .blog-content p {
    margin-bottom: 24px;
  }
  
  .blog-content img {
    max-width: 100%;
    border-radius: 8px;
    margin: 24px 0;
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
  }
  
  .code-block {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 8px;
    overflow-x: auto;
    margin: 24px 0;
    font-family: 'Consolas', 'Monaco', monospace;
    font-size: 0.9em;
  }
  
  .visualization-container {
    margin: 40px 0;
    padding: 24px;
    background: var(--card-bg);
    border-radius: 12px;
    box-shadow: 0 4px 16px rgba(0,0,0,0.08);
  }
  
  .visualization-controls {
    display: flex;
    gap: 16px;
    margin-bottom: 24px;
    flex-wrap: wrap;
  }
  
  .control-group {
    display: flex;
    flex-direction: column;
    gap: 8px;
  }
  
  button, select {
    padding: 8px 16px;
    background: var(--primary-color);
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background 0.2s;
  }
  
  button:hover {
    background: #414345;
  }
  
  .highlight {
    background: rgba(255, 179, 71, 0.2);
    padding: 2px 4px;
    border-radius: 4px;
  }
  
  .math-formula {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 8px;
    margin: 24px 0;
    overflow-x: auto;
    text-align: center;
  }
  
  .tabs {
    display: flex;
    gap: 4px;
    margin-bottom: 24px;
    border-bottom: 2px solid #eee;
    padding-bottom: 2px;
}

.tab {
    padding: 8px 16px;
    background: #f8f9fa;
    border-radius: 4px 4px 0 0;
    cursor: pointer;
    transition: all 0.2s;
    border: 1px solid #eee;
    border-bottom: none;
}

.tab:hover {
    background: #e9ecef;
}

.tab.active {
    background: var(--primary-color);
    color: white;
    border-color: var(--primary-color);
}

.tab-content {
    display: none;
    padding: 20px;
    background: #fff;
    border-radius: 4px;
    box-shadow: 0 2px 8px rgba(0,0,0,0.05);
}

.tab-content.active {
    display: block;
    animation: fadeIn 0.3s ease-in;
}

@keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

.code-block {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 4px;
    margin: 16px 0;
    font-family: 'Consolas', monospace;
}

.math-formula {
    padding: 24px;
    background: var(--code-bg);
    border-radius: 4px;
    margin: 16px 0;
    overflow-x: auto;
    text-align: center;
}
  
  .progress-container {
    height: 20px;
    width: 100%;
    background-color: #e9ecef;
    border-radius: 4px;
    margin: 16px 0;
  }
  
  .progress-bar {
    height: 100%;
    width: 0;
    background-color: var(--accent-color);
    border-radius: 4px;
    transition: width 0.3s ease;
  }
  
  .tooltip {
    position: absolute;
    padding: 8px;
    background: rgba(0,0,0,0.8);
    color: white;
    border-radius: 4px;
    pointer-events: none;
    font-size: 0.9em;
    z-index: 10;
  }
  
  @media (max-width: 768px) {
    #main {
      padding: 24px 5vw !important;
    }
    
    .visualization-controls {
      flex-direction: column;
    }
  }

  .comparison-container {
    margin: 32px 0;
    background: var(--card-bg);
    padding: 24px;
    border-radius: 12px;
    box-shadow: 0 4px 16px rgba(0,0,0,0.08);
}

.comparison-table {
    width: 100%;
    border-collapse: separate;
    border-spacing: 0;
    margin-top: 20px;
    font-size: 0.95em;
}

.comparison-table th,
.comparison-table td {
    padding: 16px;
    text-align: left;
    border: 1px solid #eee;
}

.comparison-table th {
    background: var(--primary-color);
    color: white;
    font-weight: 500;
    position: relative;
}

.comparison-table th:first-child {
    border-top-left-radius: 8px;
}

.comparison-table th:last-child {
    border-top-right-radius: 8px;
}

.comparison-table tr:last-child td:first-child {
    border-bottom-left-radius: 8px;
}

.comparison-table tr:last-child td:last-child {
    border-bottom-right-radius: 8px;
}

.comparison-table td {
    background: white;
    transition: background-color 0.2s;
}

.comparison-table tr:nth-child(even) td {
    background: #f8f9fa;
}

.comparison-table tr:hover td {
    background: #f0f0f0;
}

.comparison-table td:first-child {
    font-weight: 500;
    color: var(--primary-color);
}

/* Add responsive styles */
@media (max-width: 768px) {
    .comparison-table {
        font-size: 0.85em;
    }
    
    .comparison-table th,
    .comparison-table td {
        padding: 12px;
    }
}
  </style> </head> <body> <div id="main" style="padding: 32px 15vw;"> <a href="index.html" class="back-btn">&larr; Back to Home</a>
  <article class="blog-post">
    <h1>Everything about Autoencoders: From Linear Algebra to its Multimodal and Crossmodal Generative Capabilities</h1>
    <div class="blog-meta">May 5, 2025 -  Multimodal Unsupervised Learning -  By Soumya Savarn</div>
    
    <div class="blog-content">

      <!-- Motivation -->
      <section id="motivation">
        <h2>Motivation</h2>
        <p>My journey into generative models began with what seemed like an overwhelming field. However, my academic coursework in probability theory, Deep Learning, and Multimodal Data Processing provided the perfect foundation. Through this combined knowledge, I discovered that autoencoders offered an accessible entry point into understanding and working with Generative Multimodal models. I tried to build every concept from scratch, and you can watch this video for deeper insights into my research journey. It combines all my learnings into a single comprehensive overview. Thanks to my Multimodal Data Processing course project for rekindling my passion for sharing knowledge online!</p>
      </section>

      <!-- Historical Perspective -->
      <section id="history">
        <h2>Historical Perspective</h2>
        <p>Autoencoders trace back to early neural network research in the 1980s. Variants like denoising and variational autoencoders emerged after 2010, enabling applications in image and multimodal learning. Recent advances in cross-modal generation build on these foundations.</p>
      </section>

      <!-- Learning Takeaways -->
      <section id="learning">
        <h2>What I Learned</h2>
        <p>This work deepened my understanding of how representation learning bridges multiple data types. I learned how different constraints (sparse, variational) shape latent spaces and model capabilities.</p>
      </section>

      


      <h2>Introduction</h2>
      <p>Autoencoders are a type of artificial neural network used for unsupervised learning. Their primary goal is to learn efficient representations of input data, typically for dimensionality reduction or noise removal. The architecture consists of an <b>encoder</b> that compresses the input, a <b>bottleneck</b> that stores the compressed knowledge, and a <b>decoder</b> that reconstructs the original data from this compressed form.</p>
      
      <p>In this blog, we'll explore autoencoders from their mathematical foundations to advanced applications in multimodal learning. We'll also implement interactive visualizations to help you understand how autoencoders work.</p>
      
      <h2>Autoencoder vs U-Net: Understanding the Differences</h2>
      <div class="comparison-container">
        <p>While both architectures use encoder-decoder structures, they serve different purposes and have key differences:</p>
        
        <table class="comparison-table">
          <tr>
            <th>Feature</th>
            <th>Autoencoder</th>
            <th>U-Net</th>
          </tr>
          <tr>
            <td>Primary Purpose</td>
            <td>Unsupervised learning, dimensionality reduction, feature learning</td>
            <td>Supervised segmentation, dense prediction tasks</td>
          </tr>
          <tr>
            <td>Architecture</td>
            <td>Symmetric encoder-decoder with bottleneck</td>
            <td>Encoder-decoder with skip connections between corresponding layers</td>
          </tr>
          <tr>
            <td>Information Flow</td>
            <td>All information passes through bottleneck</td>
            <td>Features can bypass bottleneck through skip connections</td>
          </tr>
          <tr>
            <td>Output Size</td>
            <td>Same as input (reconstruction)</td>
            <td>Can be different from input (segmentation map)</td>
          </tr>
          <tr>
            <td>Training</td>
            <td>Self-supervised (input = target)</td>
            <td>Supervised (requires labeled data)</td>
          </tr>
        </table>
      </div>
      
      <div class="visualization-container">
        <h3>Autoencoder Architecture</h3>
        <img src="autoencoder-architecture.png" alt="Autoencoder Architecture showing encoder, bottleneck, and decoder" class="architecture-img">
        <p class="img-caption">Basic architecture of an autoencoder showing the encoding and decoding process</p>
      </div>
      
      <h2>Mathematical Foundation</h2>
      <p>An autoencoder is defined by two main components: an <span class="highlight">encoder function</span> that transforms the input data, and a <span class="highlight">decoder function</span> that recreates the input data from the encoded representation.</p>
      
      <p>Formally, for input space $$X$$ and encoded space $$Z$$, we define:</p>
      <div class="math-formula">
        \[ \text{Encoder}: f_\phi: \mathcal{X} \rightarrow \mathcal{Z} \]
        \[ \text{Decoder}: g_\theta: \mathcal{Z} \rightarrow \mathcal{X} \]
      </div>
      
      <p>The training objective is to minimize the reconstruction error:</p>
      <div class="math-formula">
        \[ \mathcal{L}(\phi, \theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\phi(\mathbf{x}_i))\|^2 \]
      </div>
      
      <p>Surprisingly, for linear autoencoders (using only linear activations), the optimal solution is equivalent to Principal Component Analysis (PCA). This mathematical connection is well-explained in <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/src/Lecture7/modules/Module2/Lecture7_2.pdf" target="_blank">these lecture notes from IIT Madras's Deep Learning course</a> and rigorously proven in <a href="https://arxiv.org/abs/1804.10253" target="_blank">this paper by Plaut (2018)</a>. The linear autoencoder learns to project data onto the principal subspace, just like PCA.</p>

      <div class="math-formula">
        \[ \mathcal{L}(\phi, \theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\phi(\mathbf{x}_i))\|^2 \]
        <p class="formula-caption">Linear autoencoder loss function, which under optimal conditions yields PCA solution</p>
      </div>
      
      <div class="visualization-container">
        <h2><strong>BONUS: </strong>Recirculation Algorithm</h2>
        <h4>You may skip this part.</h4>
  <p>
    It is very rare to find something related to it in mainstream machine learning community.
    The Recirculation Algorithm is an alternative to backpropagation for training neural networks, particularly autoencoders. It was introduced by Geoffrey Hinton and James McClelland in 1987 as a more biologically plausible learning mechanism. 
    (<a href="https://www.mdpi.com/2227-7390/11/8/1777?utm_source=chatgpt.com" target="_blank">MDPI</a>)
  </p>

  <h3> Key Reference</h3>
  <ul>
    <li>
      <strong>Hinton, G.E., & McClelland, J.L. (1987).</strong> 
      <em>Learning Representations by Recirculation</em>. 
      In <em>Proceedings of the Neural Information Processing Systems</em>, Denver, CO, USA. MIT Press: Cambridge, MA, USA.
      <br>
      <a href="https://www.mdpi.com/2227-7390/11/8/1777" target="_blank">Link to reference</a>
    </li>
  </ul>

  <p>
    In this foundational paper, Hinton and McClelland propose the recirculation algorithm as a method for training neural networks without the need for explicit error backpropagation. Instead, the network adjusts its weights based on the difference between the input and the reconstructed output, allowing for local learning rules that are more aligned with biological neural processes.
  </p>

  <h3> Overview of the Recirculation Algorithm</h3>
  <p>The recirculation algorithm operates in two main phases:</p>
  <ol>
    <li><strong>Forward Phase</strong>: The input data is passed through the network to produce an output.</li>
    <li><strong>Backward (Recirculation) Phase</strong>: The output is then fed back into the network as input, and the network processes this "recirculated" data to produce a reconstruction.</li>
  </ol>

  <p>
    The weights are updated based on the difference between the original input and the reconstruction, using local learning rules. This approach allows the network to learn representations by minimizing the discrepancy between the input and its reconstruction without relying on global error signals.
    (<a href="https://pubmed.ncbi.nlm.nih.gov/30317133/?utm_source=chatgpt.com" target="_blank">PubMed</a>)
  </p>

  <h3> Further Reading</h3>
  <ul>
    <li>
      <strong>Buscema, P.M.</strong> 
      <em>Recirculation Neural Networks</em>. 
      <a href="https://www.academia.edu/5743527/Recirculation_Neural_Networks" target="_blank">Link to paper</a>
    </li>
    <li>
      <strong>Baldi, P., & Sadowski, P. (2018).</strong> 
      <em>Learning in the Machine: Recirculation is Random Backpropagation</em>. 
      <a href="https://pubmed.ncbi.nlm.nih.gov/30317133/" target="_blank">Link to article</a>
    </li>
  </ul>

    <p>
      These works delve into the theoretical underpinnings of the recirculation algorithm and its relationship to other learning methods, offering valuable insights into alternative approaches to training neural networks.
    </p>

       
        <div class="math-formula">
          <h3>Forward Pass:</h3>
          \[ h = \sigma(W_1 x) \]
          \[ y = \sigma(W_2 h) \]
          <p class="formula-caption">where \(\sigma\) is the sigmoid activation function</p>
        </div>

        <div class="math-formula">
          <h3>Recirculation:</h3>
          \[ h_{rec} = \sigma(W_2^T y) \]
          <p class="formula-caption">where \(W_2^T\) is the transpose of the decoder weights</p>
        </div>

        <div class="math-formula">
          <h3>Weight Update:</h3>
          \[ \Delta W_1 = \eta (h_{rec} - h) x^T \]
          <p class="formula-caption">where \(\eta\) is the learning rate</p>
        </div>

        <h3>Key Properties:</h3>
        <ul>
          <li><strong>Local Learning:</strong> Updates only use information available at each synapse</li>
          <li><strong>Biological Plausibility:</strong> No separate error backpropagation channel needed</li>
          <li><strong>Hardware Efficiency:</strong> Well-suited for neuromorphic computing systems</li>
        </ul>

        <div class="visualization-container">
            <h2>Recirculation Algorithm Visualization</h2>
            <img src="recirculation.png" alt="Recirculation Algorithm Flow" class="architecture-img">
            <p class="img-caption">Visual representation of the recirculation algorithm showing forward pass and backward recirculation</p>
        </div>
    
          <title>Recirculation Algorithm Visualization</title>
          <style>
              body {
                  font-family: Arial, sans-serif;
                  margin: 20px;
              }
              .container {
                  display: flex;
                  flex-direction: column;
                  align-items: center;
              }
              canvas {
                  border: 1px solid #ccc;
                  margin: 10px 0;
              }
              .controls {
                  margin: 15px 0;
                  display: flex;
                  flex-direction: column;
                  align-items: center;
              }
              button {
                  padding: 8px 15px;
                  margin: 5px;
                  cursor: pointer;
              }
              .metrics {
                  margin: 10px 0;
                  padding: 10px;
                  background-color: #f5f5f5;
                  border-radius: 5px;
                  width: 600px;
              }
              .layer-info {
                  display: flex;
                  justify-content: space-around;
                  width: 600px;
                  margin-top: 10px;
              }
          </style>
     
          <div class="container">
              <h2>Recirculation Algorithm for 2→1→2 Autoencoder</h2>
              <div class="controls">
                  <button id="trainStep">Train Step</button>
                  <button id="train100">Train 100 Steps</button>
                  <button id="reset">Reset Network</button>
              </div>
              <canvas id="networkCanvas" width="600" height="300"></canvas>
              <div class="metrics">
                  <div id="errorDisplay">Reconstruction Error: 0.0</div>
                  <div id="iterationDisplay">Iterations: 0</div>
              </div>
              <div class="layer-info">
                  <div>
                      <h3>Input Data</h3>
                      <pre id="inputData"></pre>
                  </div>
                  <div>
                      <h3>Weights</h3>
                      <pre id="weightsDisplay"></pre>
                  </div>
                  <div>
                      <h3>Reconstruction</h3>
                      <pre id="reconstructionData"></pre>
                  </div>
              </div>
          </div>
      
          <script>
              // Network parameters
              const learningRate = 0.1;
              let iterations = 0;
              
              // Initialize weights with small random values for 2→1→2 architecture
              let W1 = [
                  [Math.random() * 0.2 - 0.1], // 2×1 matrix
                  [Math.random() * 0.2 - 0.1]
              ];
              
              let W2 = [
                  [Math.random() * 0.2 - 0.1], // 1×2 matrix
                  [Math.random() * 0.2 - 0.1]
              ];
              
              // Training data - simple patterns (unchanged)
              const trainingData = [
                  [0.1, 0.2],
                  [0.2, 0.4],
                  [1.0, 2.0],
                  [0.4, 0.8]
              ];
              
              let currentSampleIndex = 0;
              
              function sigmoid(x) {
                  return 1 / (1 + Math.exp(-x));
              }
              
              // Forward pass for 2→1→2 architecture
              function forwardPass(x) {
                  // Hidden layer (single node)
                  const h = sigmoid(W1[0][0] * x[0] + W1[1][0] * x[1]);
                  
                  // Output layer
                  const y = [0, 0];
                  y[0] = sigmoid(W2[0][0] * h);
                  y[1] = sigmoid(W2[1][0] * h);
                  
                  return { h, y };
              }
              
              // Recirculation for 2→1→2 architecture
              function recirculation(y) {
                  // Single hidden node reconstruction
                  const hRec = sigmoid(W2[0][0] * y[0] + W2[1][0] * y[1]);
                  return hRec;
              }
              
              // Weight update for 2→1→2 architecture
              function updateWeights(x, h, hRec) {
                  // Update W1 (2×1 matrix)
                  W1[0][0] += learningRate * (hRec - h) * x[0];
                  W1[1][0] += learningRate * (hRec - h) * x[1];
                  
                  // Update W2 (1×2 matrix)
                  W2[0][0] += learningRate * (y[0] - x[0]) * h;
                  W2[1][0] += learningRate * (y[1] - x[1]) * h;
              }
              
              // Calculate reconstruction error
              function calculateError(x, y) {
                  return Math.sqrt(Math.pow(x[0] - y[0], 2) + Math.pow(x[1] - y[1], 2));
              }
              
              // Training step
              function trainStep() {
                  // Get current training sample
                  const x = trainingData[currentSampleIndex];
                  
                  // Forward pass
                  const { h, y } = forwardPass(x);
                  
                  // Recirculation
                  const hRec = recirculation(y);
                  
                  // Update weights
                  updateWeights(x, h, hRec);
                  
                  // Calculate error
                  const error = calculateError(x, y);
                  
                  // Update displays
                  updateDisplays(x, y, error);
                  
                  // Move to next sample
                  currentSampleIndex = (currentSampleIndex + 1) % trainingData.length;
                  iterations++;
                  
                  return error;
              }
              
              // Reset network
              function resetNetwork() {
                  W1 = [
                      [Math.random() * 0.2 - 0.1], // 2×1 matrix
                      [Math.random() * 0.2 - 0.1]
                  ];
                  
                  W2 = [
                      [Math.random() * 0.2 - 0.1], // 1×2 matrix
                      [Math.random() * 0.2 - 0.1]
                  ];
                  
                  iterations = 0;
                  currentSampleIndex = 0;
                  
                  // Update displays
                  const x = trainingData[currentSampleIndex];
                  const { y } = forwardPass(x);
                  const error = calculateError(x, y);
                  updateDisplays(x, y, error);
                  
                  // Redraw network
                  drawNetwork();
              }
              
              // Update displays
              function updateDisplays(x, y, error) {
                  document.getElementById('errorDisplay').textContent = `Reconstruction Error: ${error.toFixed(4)}`;
                  document.getElementById('iterationDisplay').textContent = `Iterations: ${iterations}`;
                  document.getElementById('inputData').textContent = `[${x[0].toFixed(2)}, ${x[1].toFixed(2)}]`;
                  document.getElementById('reconstructionData').textContent = `[${y[0].toFixed(2)}, ${y[1].toFixed(2)}]`;
                  document.getElementById('weightsDisplay').textContent = 
                      `W1:\n[${W1[0][0].toFixed(2)}]\n[${W1[1][0].toFixed(2)}]\n\n` +
                      `W2:\n[${W2[0][0].toFixed(2)}]\n[${W2[1][0].toFixed(2)}]`;
                  
                  // Redraw network
                  drawNetwork();
              }
              
              // Draw network visualization
              function drawNetwork() {
                  const canvas = document.getElementById('networkCanvas');
                  const ctx = canvas.getContext('2d');
                  
                  // Clear canvas
                  ctx.clearRect(0, 0, canvas.width, canvas.height);
                  
                  // Node positions
                  const inputX = 100;
                  const hiddenX = 300;
                  const outputX = 500;
                  const nodeRadius = 20;
                  const nodeSpacing = 80;
                  
                  // Draw nodes
                  // Input layer (2 nodes)
                  ctx.fillStyle = '#A0E7E5';
                  for (let i = 0; i < 2; i++) {
                      const y = 150 + (i - 0.5) * nodeSpacing;
                      ctx.beginPath();
                      ctx.arc(inputX, y, nodeRadius, 0, 2 * Math.PI);
                      ctx.fill();
                      ctx.stroke();
                      
                      // Node label
                      ctx.fillStyle = 'black';
                      ctx.textAlign = 'center';
                      ctx.textBaseline = 'middle';
                      ctx.font = '12px Arial';
                      ctx.fillText(`x${i+1}`, inputX, y);
                      ctx.fillStyle = '#A0E7E5';
                  }
                  
                  // Hidden layer (1 node)
                  ctx.fillStyle = '#FFAEBC';
                  ctx.beginPath();
                  ctx.arc(hiddenX, 150, nodeRadius, 0, 2 * Math.PI);
                  ctx.fill();
                  ctx.stroke();
                  
                  // Hidden node label
                  ctx.fillStyle = 'black';
                  ctx.textAlign = 'center';
                  ctx.textBaseline = 'middle';
                  ctx.font = '12px Arial';
                  ctx.fillText('h', hiddenX, 150);
                  
                  // Output layer (2 nodes)
                  ctx.fillStyle = '#B4F8C8';
                  for (let i = 0; i < 2; i++) {
                      const y = 150 + (i - 0.5) * nodeSpacing;
                      ctx.beginPath();
                      ctx.arc(outputX, y, nodeRadius, 0, 2 * Math.PI);
                      ctx.fill();
                      ctx.stroke();
                      
                      // Node label
                      ctx.fillStyle = 'black';
                      ctx.textAlign = 'center';
                      ctx.textBaseline = 'middle';
                      ctx.font = '12px Arial';
                      ctx.fillText(`y${i+1}`, outputX, y);
                      ctx.fillStyle = '#B4F8C8';
                  }
                  
                  // Draw connections for 2→1→2 architecture
                  // Input to hidden connections
                  for (let i = 0; i < 2; i++) {
                      const inputY = 150 + (i - 0.5) * nodeSpacing;
                      const weight = W1[i][0];
                      const lineWidth = Math.min(Math.abs(weight) * 5, 8);
                      
                      ctx.beginPath();
                      ctx.moveTo(inputX + nodeRadius, inputY);
                      ctx.lineTo(hiddenX - nodeRadius, 150);
                      ctx.strokeStyle = weight >= 0 ? 'blue' : 'red';
                      ctx.lineWidth = lineWidth;
                      ctx.stroke();
                  }
                  
                  // Hidden to output connections
                  for (let i = 0; i < 2; i++) {
                      const outputY = 150 + (i - 0.5) * nodeSpacing;
                      const weight = W2[i][0];
                      const lineWidth = Math.min(Math.abs(weight) * 5, 8);
                      
                      ctx.beginPath();
                      ctx.moveTo(hiddenX + nodeRadius, 150);
                      ctx.lineTo(outputX - nodeRadius, outputY);
                      ctx.strokeStyle = weight >= 0 ? 'blue' : 'red';
                      ctx.lineWidth = lineWidth;
                      ctx.stroke();
                  }
                  
                  // Reset stroke style
                  ctx.strokeStyle = 'black';
                  ctx.lineWidth = 1;
              }
              
              // Event listeners
              document.getElementById('trainStep').addEventListener('click', trainStep);
              
              document.getElementById('train100').addEventListener('click', function() {
                  let totalError = 0;
                  for (let i = 0; i < 100; i++) {
                      totalError += trainStep();
                  }
                  console.log(`Average error after 100 steps: ${(totalError / 100).toFixed(4)}`);
              });
              
              document.getElementById('reset').addEventListener('click', resetNetwork);
              
              // Initialize
              resetNetwork();
          </script>
      
    </div>


      <h2>Types of Autoencoders</h2>
      <p>There are several variations of autoencoders, each with specific applications:</p>
      
      <div class="tabs">
        <div class="tab active" data-tab="vanilla">Vanilla</div>
        <div class="tab" data-tab="sparse">Sparse</div>
        <div class="tab" data-tab="denoising">Denoising</div>
        <div class="tab" data-tab="variational">Variational (VAE)</div>
        <div class="tab" data-tab="convolutional">Convolutional</div>
      </div>
      
      <div class="tab-content active" id="vanilla-content">
        <h3>Vanilla Autoencoder</h3>
        <p>The basic autoencoder architecture consists of an encoder and decoder with fully connected layers. The encoder compresses the input to a lower-dimensional code, and the decoder reconstructs the input from this code.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\theta(\mathbf{x}_i))\|^2 \]
          <p class="formula-caption">where \(f_\theta\) is the encoder and \(g_\theta\) is the decoder</p>
        </div>
      </div>
      
      <div class="tab-content" id="sparse-content">
        <h3>Sparse Autoencoder</h3>
        <p>Sparse autoencoders add a sparsity constraint to the hidden layer, forcing the model to activate only a small number of neurons at a time. This encourages the model to learn more robust features.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\theta(\mathbf{x}_i))\|^2 + \lambda \sum_{j=1}^m |\rho - \hat{\rho_j}| \]
          <p class="formula-caption">where \(\rho\) is the target sparsity and \(\hat{\rho_j}\) is the average activation of hidden unit j</p>
        </div>
      </div>
      
      <div class="tab-content" id="denoising-content">
        <h3>Denoising Autoencoder</h3>
        <p>Denoising autoencoders are trained to reconstruct clean inputs from corrupted versions. This makes them robust to noise and helps them learn more useful features.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\theta(\tilde{\mathbf{x}}_i))\|^2 \]
          <p class="formula-caption">where \(\tilde{\mathbf{x}}_i = \mathbf{x}_i + \epsilon\) is the corrupted input with noise \(\epsilon\)</p>
        </div>
      </div>
      
      <div class="tab-content" id="variational-content">
        <h3>Variational Autoencoder (VAE)</h3>
        <p>Variational autoencoders are probabilistic models that learn a latent variable model for the input data. Instead of encoding an input as a single point, they encode it as a distribution over the latent space.</p>
        <p>The VAE loss function has two components: the reconstruction loss and the KL divergence between the encoder's distribution and a prior distribution.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\phi, \theta, \mathbf{x}) = -\mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})] + D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})) \]
        </div>
      </div>
      
      <div class="tab-content" id="convolutional-content">
        <h3>Convolutional Autoencoder</h3>
        <p>Convolutional autoencoders use convolutional layers in both the encoder and decoder, making them well-suited for image data.</p>
        <div class="math-formula">
          \[ f_\theta(\mathbf{x}) = \sigma(\text{Conv2D}(\mathbf{x}) \ast \mathbf{W} + \mathbf{b}) \]
          \[ g_\theta(\mathbf{z}) = \sigma(\text{Conv2DTranspose}(\mathbf{z}) \ast \mathbf{W}' + \mathbf{b}') \]
          <p class="formula-caption">where \(W, W'\) are learnable filters and \(\sigma\) is an activation function</p>
        </div>
      </div>

      <p>For a comprehensive theoretical background on Autoencoders and Variational Autoencoders (VAEs), I highly recommend Chapter 14 and 20 of <a href="https://www.deeplearningbook.org/" target="_blank">Deep Learning</a> by Goodfellow, Bengio, and Courville.</p>

      <h2>Applications</h2>
      <p>Autoencoders serve multiple purposes in machine learning and data science. They excel at dimensionality reduction by compressing high-dimensional data while preserving key features. Their ability to learn normal data patterns makes them effective for anomaly detection through reconstruction error analysis. In image processing, denoising autoencoders can clean corrupted images. The encoded representations provide valuable features for downstream tasks like classification. Additionally, variants like variational autoencoders enable generative modeling, creating new data samples that mirror the training distribution.</p>

      <p>Let's explore <strong>Variational Autoencoders</strong> (VAEs) and their fascinating applications in multimodal learning. For a rigorous mathematical treatment and detailed proofs, I highly recommend referring to <a href="https://arxiv.org/pdf/1606.05908" target="_blank">this seminal paper</a> on multimodal VAEs.</p>

       <!-- Convex Interpolation in VAEs -->
<section id="convex-interpolation">
  <h2>Convex Interpolation in Variational Autoencoders</h2>
  <p>
    I’ve found that (obviously the researchers) in a VAE, because we force the latent variable \(z\) to follow a simple prior like \(p(z)=\mathcal{N}(0,I)\), the resulting latent space becomes (approximately) convex—any linear mix 
    \[
      \mathbf{z}_\alpha = (1-\alpha)\mathbf{z}_1 + \alpha\mathbf{z}_2,\quad \alpha\in[0,1]
    \]
    stays in a high–density region where the decoder can produce meaningful outputs .
  </p>
  <p>
    Concretely, decoding \(\mathbf{z}_\alpha\) via \(\mathbf{x}_\alpha = g_\theta(\mathbf{z}_\alpha)\) often yields smooth “in‑between” samples that semantically blend the endpoints .  
  </p>
  <p>
    However, naive latent interpolations can stray off the true data manifold, leading to artifacts. Recent work shapes the latent manifold to be locally convex—e.g., by adding a regularizer that encourages
    \[
      \|g_\theta((1-\alpha)\mathbf{z}_i + \alpha \mathbf{z}_j) - ((1-\alpha)\mathbf{x}_i + \alpha \mathbf{x}_j)\|\;\text{to be small}
    \]
    for many \(\alpha\) .  
  </p>
  <p>
    Statisticians call evaluation inside the convex hull of training points “interpolation,” which is generally safer than extrapolating outside that hull .
  </p>
</section>

<!-- Insights from “Multimodal Generative Models for Scalable Weakly‑Supervised Learning” -->
<section id="mvae-motivation">
  <h2>Moving to multimodal generation</h2>
  <p>
    By now you must have good the clue that the VAE is a generative model.
    It can generate new data points by sampling from the latent space and decoding them back to the original space. 
    This property is particularly useful in multimodal learning, where we want 
    to generate data that combines information from multiple modalities.
    A naive way to do this is to concatenate the latent variables from each modality and train a single VAE on the combined data.
    However, this approach has several limitations:
  </p>
  <ul>
    <li>It requires a large amount of labeled data for each modality, which is often not available.</li>
    <li>It does not leverage the relationships between modalities, leading to suboptimal representations.</li>
    <li>It can be computationally expensive and slow to train.</li>
  </ul>
</section>

<!-- Insights from “Multimodal Generative Models for Scalable Weakly‑Supervised Learning” -->
<div>
  <h2>Insights from “Multimodal Generative Models for Scalable Weakly‑Supervised Learning”</h2>

  <p>I first encountered the MVAE paper by Mike Wu and Noah Goodman when I was looking for a way to merge different data types—like images, text, and audio—into one shared hidden representation called <em>z</em>. In their approach, each modality’s encoder outputs a Gaussian “expert” over <em>z</em>, and they multiply those experts together along with a simple Gaussian prior. This trick, known as the product‑of‑experts (PoE), means that as I feed in more modalities, the combined Gaussian becomes tighter (its variance shrinks), so I become more confident about the value of <em>z</em> when I have richer information.</p>

  <p>Behind the scenes, training the MVAE means maximizing a modified evidence lower bound (ELBO). On one hand, I want to reconstruct each modality from <em>z</em>, which pushes up the sum of log‑likelihoods for every modality. On the other hand, I add a KL divergence term that keeps my fused posterior close to the standard normal prior, preventing the model from becoming overconfident. To handle missing data, the authors alternate between fully observed examples—where all encoders and the PoE get updated—and partially observed examples—where only the single‑modality encoder for the available data is updated. This sub‑sampling trick makes sure I can still infer a sensible <em>z</em> even if some streams drop out.</p>

  <p>An elegant aspect of MVAE is its shared decoder. Instead of building one large decoder per modality, they attach small “heads” for each output type to a single backbone network. This reduces the total number of parameters and forces the model to learn a core latent space that works across all data types. When I ran experiments on tasks like predicting CelebA attributes from face images and captions, or translating between English and French sentences, the MVAE matched or beat state‑of‑the‑art methods while using far fewer weights.</p>

  <h3>How the Product‑of‑Experts Works</h3>
  <p>Imagine I have a standard normal prior <em>p(z)=N(0,1)</em> and two modality encoders giving Gaussian posteriors: from an image encoder I get <em>N(μ₁=2, σ₁²=0.25)</em>, and from a text encoder I get <em>N(μ₂=4, σ₂²=1)</em>. To combine them, I convert variances into precisions (inverse variance): τ₀=1, τ₁=4, τ₂=1, so the total precision is τ*=6. That gives me a combined variance σ*²=1/6≈0.167. The fused mean is (τ₀⋅0 + τ₁⋅2 + τ₂⋅4)/(τ*) =12/6=2. So my product‑of‑experts posterior becomes <em>N(2, 0.167)</em>. Notice how 0.167 is smaller than each individual variance—that’s the sharpening effect of PoE when I have both modalities.</p>

  <h3>Shared Decoder and Parameter Efficiency</h3>
  <p>In the decoder, I use one neural network backbone and then branch off small heads for each modality. This means fewer total parameters compared to having a completely separate decoder per modality. Sharing parameters in this way ensures the latent code <em>z</em> captures features relevant to all data types, rather than letting each decoder learn its own private tricks.</p>

  <h3>Training with Missing Data</h3>
  <p>Real‑world datasets often have gaps, so MVAE trains on two kinds of examples. When all modalities are present, I update both the PoE encoder and every unimodal encoder. When only one modality is present, I update just that encoder. This alternation makes the model robust: it learns to fuse rich multimodal inputs and also to handle cases when only a single view is available.</p>

  <h3>Results and Applications</h3>
  <p>When I applied MVAE to tasks like face‑attribute prediction (CelebA), synthetic image transforms (e.g., rotations), and bilingual translation (English↔French), the model consistently learned a coherent joint representation. I could predict missing modalities and translate between them, all with fewer parameters than comparable methods.</p>

  <p>Working through these ideas helped me see why product‑of‑experts sharpens uncertainty and why sharing decoders yields a compact, unified latent space. The sub‑sampling training trick then ties it all together, making MVAE a powerful tool for real‑world multimodal data that often arrives incomplete.</p>
</div>


      <script>
        // Initialize tab functionality
        document.addEventListener('DOMContentLoaded', () => {
            const tabs = document.querySelectorAll('.tab');
            
            tabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    // Remove active class from all tabs and content
                    tabs.forEach(t => t.classList.remove('active'));
                    document.querySelectorAll('.tab-content').forEach(content => {
                        content.classList.remove('active');
                    });
                    
                    // Add active class to clicked tab and corresponding content
                    tab.classList.add('active');
                    const contentId = `${tab.dataset.tab}-content`;
                    document.getElementById(contentId).classList.add('active');
                });
            });
        });
        </script>

      

      
     
    </div>
  </article>
  </div>  
  
  </body>
   </html>