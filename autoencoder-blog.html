<!DOCTYPE html>
 <html lang="en">
  <head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width,initial-scale=1.0"> <title>Understanding Autoencoders | Soumya Savarn</title> 
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
  <script src="https://cdn.jsdelivr.net/npm/tensorflow@2.8.0/dist/tf.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
  <style> :root { --primary-color: #232526; --accent-color: #ffb347; --bg-color: #f8f9fa; --text-color: #333; --light-text: #666; --card-bg: #fff; --code-bg: #f1f1f1; }
  body {
    font-family: 'Segoe UI', Arial, sans-serif;
    margin: 0;
    background: var(--bg-color);
    color: var(--text-color);
    line-height: 1.8;
  }
  
  .back-btn {
    display: inline-block;
    padding: 8px 16px;
    color: var(--primary-color);
    text-decoration: none;
    margin-bottom: 32px;
    transition: color 0.2s;
    font-weight: 500;
    border-radius: 4px;
    background: rgba(255, 179, 71, 0.1);
  }
  
  .back-btn:hover {
    color: var(--accent-color);
    background: rgba(255, 179, 71, 0.2);
  }
  
  .blog-post {
    max-width: 800px;
    margin: 0 auto;
  }
  
  .blog-meta {
    color: var(--light-text);
    margin: 16px 0 32px;
    font-size: 0.95em;
  }
  
  .blog-content {
    line-height: 1.8;
    font-size: 1.1em;
  }
  
  .blog-content h2 {
    margin-top: 48px;
    color: var(--primary-color);
    border-bottom: 2px solid var(--accent-color);
    padding-bottom: 8px;
    display: inline-block;
  }
  
  .blog-content h3 {
    margin-top: 32px;
    color: var(--primary-color);
  }
  
  .blog-content p {
    margin-bottom: 24px;
  }
  
  .blog-content img {
    max-width: 100%;
    border-radius: 8px;
    margin: 24px 0;
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
  }
  
  .code-block {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 8px;
    overflow-x: auto;
    margin: 24px 0;
    font-family: 'Consolas', 'Monaco', monospace;
    font-size: 0.9em;
  }
  
  .visualization-container {
    margin: 40px 0;
    padding: 24px;
    background: var(--card-bg);
    border-radius: 12px;
    box-shadow: 0 4px 16px rgba(0,0,0,0.08);
  }
  
  .visualization-controls {
    display: flex;
    gap: 16px;
    margin-bottom: 24px;
    flex-wrap: wrap;
  }
  
  .control-group {
    display: flex;
    flex-direction: column;
    gap: 8px;
  }
  
  button, select {
    padding: 8px 16px;
    background: var(--primary-color);
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background 0.2s;
  }
  
  button:hover {
    background: #414345;
  }
  
  .highlight {
    background: rgba(255, 179, 71, 0.2);
    padding: 2px 4px;
    border-radius: 4px;
  }
  
  .math-formula {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 8px;
    margin: 24px 0;
    overflow-x: auto;
    text-align: center;
  }
  
  .tabs {
    display: flex;
    gap: 4px;
    margin-bottom: 24px;
    border-bottom: 2px solid #eee;
    padding-bottom: 2px;
}

.tab {
    padding: 8px 16px;
    background: #f8f9fa;
    border-radius: 4px 4px 0 0;
    cursor: pointer;
    transition: all 0.2s;
    border: 1px solid #eee;
    border-bottom: none;
}

.tab:hover {
    background: #e9ecef;
}

.tab.active {
    background: var(--primary-color);
    color: white;
    border-color: var(--primary-color);
}

.tab-content {
    display: none;
    padding: 20px;
    background: #fff;
    border-radius: 4px;
    box-shadow: 0 2px 8px rgba(0,0,0,0.05);
}

.tab-content.active {
    display: block;
    animation: fadeIn 0.3s ease-in;
}

@keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

.code-block {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 4px;
    margin: 16px 0;
    font-family: 'Consolas', monospace;
}

.math-formula {
    padding: 24px;
    background: var(--code-bg);
    border-radius: 4px;
    margin: 16px 0;
    overflow-x: auto;
    text-align: center;
}
  
  .progress-container {
    height: 20px;
    width: 100%;
    background-color: #e9ecef;
    border-radius: 4px;
    margin: 16px 0;
  }
  
  .progress-bar {
    height: 100%;
    width: 0;
    background-color: var(--accent-color);
    border-radius: 4px;
    transition: width 0.3s ease;
  }
  
  .tooltip {
    position: absolute;
    padding: 8px;
    background: rgba(0,0,0,0.8);
    color: white;
    border-radius: 4px;
    pointer-events: none;
    font-size: 0.9em;
    z-index: 10;
  }
  
  @media (max-width: 768px) {
    #main {
      padding: 24px 5vw !important;
    }
    
    .visualization-controls {
      flex-direction: column;
    }
  }

  .comparison-container {
    margin: 32px 0;
    background: var(--card-bg);
    padding: 24px;
    border-radius: 12px;
    box-shadow: 0 4px 16px rgba(0,0,0,0.08);
}

.comparison-table {
    width: 100%;
    border-collapse: separate;
    border-spacing: 0;
    margin-top: 20px;
    font-size: 0.95em;
}

.comparison-table th,
.comparison-table td {
    padding: 16px;
    text-align: left;
    border: 1px solid #eee;
}

.comparison-table th {
    background: var(--primary-color);
    color: white;
    font-weight: 500;
    position: relative;
}

.comparison-table th:first-child {
    border-top-left-radius: 8px;
}

.comparison-table th:last-child {
    border-top-right-radius: 8px;
}

.comparison-table tr:last-child td:first-child {
    border-bottom-left-radius: 8px;
}

.comparison-table tr:last-child td:last-child {
    border-bottom-right-radius: 8px;
}

.comparison-table td {
    background: white;
    transition: background-color 0.2s;
}

.comparison-table tr:nth-child(even) td {
    background: #f8f9fa;
}

.comparison-table tr:hover td {
    background: #f0f0f0;
}

.comparison-table td:first-child {
    font-weight: 500;
    color: var(--primary-color);
}

/* Add responsive styles */
@media (max-width: 768px) {
    .comparison-table {
        font-size: 0.85em;
    }
    
    .comparison-table th,
    .comparison-table td {
        padding: 12px;
    }
}
  </style> </head> <body> <div id="main" style="padding: 32px 15vw;"> <a href="index.html" class="back-btn">&larr; Back to Home</a>
  <article class="blog-post">
    <h1>Everything about Autoencoders: From Linear Algebra to its Multimodal and Crossmodal Generative Capabilities</h1>
    <div class="blog-meta">May 5, 2025 -  Multimodal Unsupervised Learning -  By Soumya Savarn</div>
    
    <div class="blog-content">

      <!-- Motivation -->
      <section id="motivation">
        <h2>Motivation</h2>
        <p>My journey into generative models began with what seemed like an overwhelming field. However, my academic coursework in probability theory, Deep Learning, and Multimodal Data Processing provided the perfect foundation. Through this combined knowledge, I discovered that autoencoders offered an accessible entry point into understanding and working with Generative Multimodal models. I tried to build every concept from scratch, and you can watch this video for deeper insights into my research journey. It combines all my learnings into a single comprehensive overview. Thanks to my Multimodal Data Processing course project for rekindling my passion for sharing knowledge online!</p>
      </section>

      <!-- Historical Perspective -->
      <section id="history">
        <h2>Historical Perspective</h2>
        <p>Autoencoders trace back to early neural network research in the 1980s. Variants like denoising and variational autoencoders emerged after 2010, enabling applications in image and multimodal learning. Recent advances in cross-modal generation build on these foundations.</p>
      </section>

      <!-- Learning Takeaways -->
      <section id="learning">
        <h2>What I Learned</h2>
        <p>This work deepened my understanding of how representation learning bridges multiple data types. I learned how different constraints (sparse, variational) shape latent spaces and model capabilities.</p>
      </section>

      


      <h2>Introduction</h2>
      <p>Autoencoders are a type of artificial neural network used for unsupervised learning. Their primary goal is to learn efficient representations of input data, typically for dimensionality reduction or noise removal. The architecture consists of an <b>encoder</b> that compresses the input, a <b>bottleneck</b> that stores the compressed knowledge, and a <b>decoder</b> that reconstructs the original data from this compressed form.</p>
      
      <p>In this blog, we'll explore autoencoders from their mathematical foundations to advanced applications in multimodal learning. We'll also implement interactive visualizations to help you understand how autoencoders work.</p>
      
      <h2>Autoencoder vs U-Net: Understanding the Differences</h2>
      <div class="comparison-container">
        <p>While both architectures use encoder-decoder structures, they serve different purposes and have key differences:</p>
        
        <table class="comparison-table">
          <tr>
            <th>Feature</th>
            <th>Autoencoder</th>
            <th>U-Net</th>
          </tr>
          <tr>
            <td>Primary Purpose</td>
            <td>Unsupervised learning, dimensionality reduction, feature learning</td>
            <td>Supervised segmentation, dense prediction tasks</td>
          </tr>
          <tr>
            <td>Architecture</td>
            <td>Symmetric encoder-decoder with bottleneck</td>
            <td>Encoder-decoder with skip connections between corresponding layers</td>
          </tr>
          <tr>
            <td>Information Flow</td>
            <td>All information passes through bottleneck</td>
            <td>Features can bypass bottleneck through skip connections</td>
          </tr>
          <tr>
            <td>Output Size</td>
            <td>Same as input (reconstruction)</td>
            <td>Can be different from input (segmentation map)</td>
          </tr>
          <tr>
            <td>Training</td>
            <td>Self-supervised (input = target)</td>
            <td>Supervised (requires labeled data)</td>
          </tr>
        </table>
      </div>
      
      <div class="visualization-container">
        <h3>Autoencoder Architecture</h3>
        <img src="autoencoder-architecture.png" alt="Autoencoder Architecture showing encoder, bottleneck, and decoder" class="architecture-img">
        <p class="img-caption">Basic architecture of an autoencoder showing the encoding and decoding process</p>
      </div>
      
      <h2>Mathematical Foundation</h2>
      <p>An autoencoder is defined by two main components: an <span class="highlight">encoder function</span> that transforms the input data, and a <span class="highlight">decoder function</span> that recreates the input data from the encoded representation.</p>
      
      <p>Formally, for input space $$X$$ and encoded space $$Z$$, we define:</p>
      <div class="math-formula">
        $$ \text{Encoder}: f_\phi: X \rightarrow Z $$
        $$ \text{Decoder}: g_\theta: Z \rightarrow X $$
      </div>
      
      <p>The training objective is to minimize the reconstruction error:</p>
      <div class="math-formula">
        $$ \mathcal{L}(\phi, \theta) = \sum_{x \in X} ||x - g_\theta(f_\phi(x))||^2 $$
      </div>
      
      <p>For linear autoencoders, this has an elegant solution related to Principal Component Analysis (PCA). The optimal linear autoencoder learns to project the data onto the principal subspace of the data.</p>
      
      <div class="visualization-container">
        <h2><strong>BONUS: </strong>Recirculation Algorithm</h2>
        <h4>You may skip this part.</h4>
  <p>
    It is very rare to find something related to it in mainstream machine learning community.
    The Recirculation Algorithm is an alternative to backpropagation for training neural networks, particularly autoencoders. It was introduced by Geoffrey Hinton and James McClelland in 1987 as a more biologically plausible learning mechanism. 
    (<a href="https://www.mdpi.com/2227-7390/11/8/1777?utm_source=chatgpt.com" target="_blank">MDPI</a>)
  </p>

  <h3> Key Reference</h3>
  <ul>
    <li>
      <strong>Hinton, G.E., & McClelland, J.L. (1987).</strong> 
      <em>Learning Representations by Recirculation</em>. 
      In <em>Proceedings of the Neural Information Processing Systems</em>, Denver, CO, USA. MIT Press: Cambridge, MA, USA.
      <br>
      <a href="https://www.mdpi.com/2227-7390/11/8/1777" target="_blank">Link to reference</a>
    </li>
  </ul>

  <p>
    In this foundational paper, Hinton and McClelland propose the recirculation algorithm as a method for training neural networks without the need for explicit error backpropagation. Instead, the network adjusts its weights based on the difference between the input and the reconstructed output, allowing for local learning rules that are more aligned with biological neural processes.
  </p>

  <h3> Overview of the Recirculation Algorithm</h3>
  <p>The recirculation algorithm operates in two main phases:</p>
  <ol>
    <li><strong>Forward Phase</strong>: The input data is passed through the network to produce an output.</li>
    <li><strong>Backward (Recirculation) Phase</strong>: The output is then fed back into the network as input, and the network processes this "recirculated" data to produce a reconstruction.</li>
  </ol>

  <p>
    The weights are updated based on the difference between the original input and the reconstruction, using local learning rules. This approach allows the network to learn representations by minimizing the discrepancy between the input and its reconstruction without relying on global error signals.
    (<a href="https://pubmed.ncbi.nlm.nih.gov/30317133/?utm_source=chatgpt.com" target="_blank">PubMed</a>)
  </p>

  <h3> Further Reading</h3>
  <ul>
    <li>
      <strong>Buscema, P.M.</strong> 
      <em>Recirculation Neural Networks</em>. 
      <a href="https://www.academia.edu/5743527/Recirculation_Neural_Networks" target="_blank">Link to paper</a>
    </li>
    <li>
      <strong>Baldi, P., & Sadowski, P. (2018).</strong> 
      <em>Learning in the Machine: Recirculation is Random Backpropagation</em>. 
      <a href="https://pubmed.ncbi.nlm.nih.gov/30317133/" target="_blank">Link to article</a>
    </li>
  </ul>

    <p>
      These works delve into the theoretical underpinnings of the recirculation algorithm and its relationship to other learning methods, offering valuable insights into alternative approaches to training neural networks.
    </p>

       
        <div class="math-formula">
          <h3>Forward Pass:</h3>
          $$ h = \sigma(W_1 x) $$
          $$ y = \sigma(W_2 h) $$
          <p class="formula-caption">where \(\sigma\) is the sigmoid activation function</p>
        </div>

        <div class="math-formula">
          <h3>Recirculation:</h3>
          $$ h_{rec} = \sigma(W_2^T y) $$
          <p class="formula-caption">where \(W_2^T\) is the transpose of the decoder weights</p>
        </div>

        <div class="math-formula">
          <h3>Weight Update:</h3>
          $$ \Delta W_1 = \eta (h_{rec} - h) x^T $$
          <p class="formula-caption">where \(\eta\) is the learning rate</p>
        </div>

        <h3>Key Properties:</h3>
        <ul>
          <li><strong>Local Learning:</strong> Updates only use information available at each synapse</li>
          <li><strong>Biological Plausibility:</strong> No separate error backpropagation channel needed</li>
          <li><strong>Hardware Efficiency:</strong> Well-suited for neuromorphic computing systems</li>
        </ul>

        <div class="code-block">
          <p>Network Architecture: 2→2→2</p>
          <ul>
            <li>Input layer (x): 2 nodes</li>
            <li>Hidden layer (h): 2 nodes</li>
            <li>Output layer (y): 2 nodes</li>
          </ul>
        </div>
      </div>
      
      <h2>Types of Autoencoders</h2>
      <p>There are several variations of autoencoders, each with specific applications:</p>
      
      <div class="tabs">
        <div class="tab active" data-tab="vanilla">Vanilla</div>
        <div class="tab" data-tab="sparse">Sparse</div>
        <div class="tab" data-tab="denoising">Denoising</div>
        <div class="tab" data-tab="variational">Variational (VAE)</div>
        <div class="tab" data-tab="convolutional">Convolutional</div>
      </div>
      
      <div class="tab-content active" id="vanilla-content">
        <h3>Vanilla Autoencoder</h3>
        <p>The basic autoencoder architecture consists of an encoder and decoder with fully connected layers. The encoder compresses the input to a lower-dimensional code, and the decoder reconstructs the input from this code.</p>
        <div class="math-formula">
          $$ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N ||x_i - g_\theta(f_\theta(x_i))||^2 $$
          <p class="formula-caption">where \(f_\theta\) is the encoder and \(g_\theta\) is the decoder</p>
        </div>
      </div>
      
      <div class="tab-content" id="sparse-content">
        <h3>Sparse Autoencoder</h3>
        <p>Sparse autoencoders add a sparsity constraint to the hidden layer, forcing the model to activate only a small number of neurons at a time. This encourages the model to learn more robust features.</p>
        <div class="math-formula">
          $$ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N ||x_i - g_\theta(f_\theta(x_i))||^2 + \lambda \sum_{j=1}^m |\rho - \hat{\rho_j}| $$
          <p class="formula-caption">where \(\rho\) is the target sparsity and \(\hat{\rho_j}\) is the average activation of hidden unit j</p>
        </div>
      </div>
      
      <div class="tab-content" id="denoising-content">
        <h3>Denoising Autoencoder</h3>
        <p>Denoising autoencoders are trained to reconstruct clean inputs from corrupted versions. This makes them robust to noise and helps them learn more useful features.</p>
        <div class="math-formula">
          $$ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N ||x_i - g_\theta(f_\theta(\tilde{x_i}))||^2 $$
          <p class="formula-caption">where \(\tilde{x_i} = x_i + \epsilon\) is the corrupted input with noise \(\epsilon\)</p>
        </div>
      </div>
      
      <div class="tab-content" id="variational-content">
        <h3>Variational Autoencoder (VAE)</h3>
        <p>Variational autoencoders are probabilistic models that learn a latent variable model for the input data. Instead of encoding an input as a single point, they encode it as a distribution over the latent space.</p>
        <p>The VAE loss function has two components: the reconstruction loss and the KL divergence between the encoder's distribution and a prior distribution.</p>
        <div class="math-formula">
          $$ \mathcal{L}(\phi, \theta, x) = -\mathbb{E}_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)] + D_{KL}(q_\phi(z|x) || p(z)) $$
        </div>
      </div>
      
      <div class="tab-content" id="convolutional-content">
        <h3>Convolutional Autoencoder</h3>
        <p>Convolutional autoencoders use convolutional layers in both the encoder and decoder, making them well-suited for image data.</p>
        <div class="math-formula">
          $$ f_\theta(x) = \sigma(\text{Conv2D}(x) * W + b) $$
          $$ g_\theta(z) = \sigma(\text{Conv2DTranspose}(z) * W' + b') $$
          <p class="formula-caption">where \(W, W'\) are learnable filters and \(\sigma\) is an activation function</p>
        </div>
      </div>
      
      <h2>Applications</h2>
      <p>Autoencoders have a wide range of applications in machine learning and data science:</p>
      
      <ul>
        <li><strong>Dimensionality Reduction:</strong> Autoencoders can compress high-dimensional data into a lower-dimensional representation while preserving important features.</li>
        <li><strong>Anomaly Detection:</strong> By learning the normal patterns in data, autoencoders can identify anomalies as inputs with high reconstruction error.</li>
        <li><strong>Image Denoising:</strong> Denoising autoencoders can remove noise from images by learning to reconstruct clean images from noisy ones.</li>
        <li><strong>Feature Learning:</strong> The encoded representations can be used as features for downstream tasks like classification.</li>
        <li><strong>Generative Modeling:</strong> Variational autoencoders can generate new data samples similar to the training data.</li>
      </ul>


      <!-- Convex Interpolation in VAEs -->
<section id="convex-interpolation">
  <h2>Convex Interpolation in VAEs</h2>
  <p>
    I’ve found that in a VAE, because we force the latent variable \(z\) to follow a simple prior like \(p(z)=\mathcal{N}(0,I)\), the resulting latent space becomes (approximately) convex—any linear mix 
    \[
      z_\alpha = (1-\alpha)\,z_1 + \alpha\,z_2,\quad \alpha\in[0,1]
    \]
    stays in a high–density region where the decoder can produce meaningful outputs :contentReference[oaicite:0]{index=0}.
  </p>
  <p>
    Concretely, decoding \(z_\alpha\) via \(x_\alpha = g_\theta(z_\alpha)\) often yields smooth “in‑between” samples that semantically blend the endpoints :contentReference[oaicite:1]{index=1}.  
  </p>
  <p>
    However, naive latent interpolations can stray off the true data manifold, leading to artifacts. Recent work shapes the latent manifold to be locally convex—e.g., by adding a regularizer that encourages
    \[
      \|g_\theta((1-\alpha)z_i + \alpha z_j) - ((1-\alpha)x_i + \alpha x_j)\|\;\text{to be small}
    \]
    for many \(\alpha\) :contentReference[oaicite:2]{index=2}.  
  </p>
  <p>
    Statisticians call evaluation inside the convex hull of training points “interpolation,” which is generally safer than extrapolating outside that hull :contentReference[oaicite:3]{index=3}.
  </p>
</section>

<!-- Convex Interpolation in Variational Autoencoders -->
<section id="vae-convex">
  <h2>Convex Interpolation in Variational Autoencoders</h2>
  <p>
    I’ve learned that variational autoencoders (VAEs) impose a prior \(p(z)=\mathcal{N}(0,I)\) which causes the latent space to approximate a convex manifold, making linear combinations
    \[
      z_\alpha = (1-\alpha)\,z_1 + \alpha\,z_2,\quad \alpha\in[0,1]
    \]
    lie in regions where decoding yields meaningful outputs. :contentReference[oaicite:0]{index=0}
  </p>
  <p>
    By decoding \(x_\alpha = g_\theta(z_\alpha)\), we obtain smooth interpolations that blend the semantic features of the endpoints. :contentReference[oaicite:1]{index=1}
  </p>
  <p>
    However, naive interpolation can stray off the true data manifold and introduce artifacts; remedies include regularizers that encourage
    \[
      \bigl\|g_\theta((1-\alpha)z_i + \alpha z_j) - ((1-\alpha)x_i + \alpha x_j)\bigr\|
    \]
    to be small. :contentReference[oaicite:2]{index=2}
  </p>
  <p>
    Such convexity constraints help align the latent manifold with the true data distribution, improving both interpolation quality and reconstruction fidelity. :contentReference[oaicite:3]{index=3}
  </p>
</section>

<!-- Insights from “Multimodal Generative Models for Scalable Weakly‑Supervised Learning” -->
<section id="mvae-paper">
  <h2>Insights from “Multimodal Generative Models for Scalable Weakly‑Supervised Learning”</h2>
  <p>
    Wu & Goodman introduce the Multimodal VAE (MVAE), which fuses any subset of \(M\) modalities via a product‑of‑experts encoder
    \[
      q_\phi\bigl(z \mid \{x^{(i)}\}_{i\in\mathcal{O}}\bigr)
      \propto p(z)\,\prod_{i\in\mathcal{O}} q_{\phi_i}\bigl(z\mid x^{(i)}\bigr).
    \] :contentReference[oaicite:4]{index=4}
  </p>
  <p>
    Their training objective generalizes the ELBO to multiple modalities:
    \[
      \mathcal{L}(\theta,\phi)
      = \mathbb{E}_{q_\phi(z\mid x)}
        \Bigl[\sum_{i=1}^M \log p_\theta\bigl(x^{(i)}\mid z\bigr)\Bigr]
        \;-\;\mathrm{KL}\bigl(q_\phi(z\mid x)\,\|\,p(z)\bigr),
    \]
    balancing reconstruction across all modalities against regularization to the prior. :contentReference[oaicite:5]{index=5}
  </p>
  <p>
    For Gaussian encoders \(q_i(z)=\mathcal{N}(\mu_i,\Sigma_i)\) and prior \(p(z)=\mathcal{N}(0,\Sigma_0)\), the PoE posterior remains Gaussian with
    \[
      \Sigma_*^{-1} = \Sigma_0^{-1} + \sum_{i\in\mathcal{O}} \Sigma_i^{-1},
      \quad
      \mu_* = \Sigma_* \Bigl(\sum_{i\in\mathcal{O}} \Sigma_i^{-1}\mu_i\Bigr),
    \]
    which sharpens as more modalities contribute. :contentReference[oaicite:6]{index=6}
  </p>
  <p>
    They share decoder parameters \(p_\theta(x^{(i)}\mid z)\) across modalities, drastically reducing parameter count while matching state‑of‑the‑art on tasks like multimodal translation. :contentReference[oaicite:7]{index=7}
  </p>
  <p>
    A sub‑sampled training paradigm alternates between fully‑observed examples (training both uni‑ and multimodal encoders) and partially‑observed examples (training each unimodal encoder), ensuring robustness to missing data. :contentReference[oaicite:8]{index=8}
  </p>
  <p>
    Experiments on CelebA attributes, synthesized image‐transformation modalities, and bilingual translation confirm the MVAE’s ability to learn coherent joint representations under weak supervision. :contentReference[oaicite:9]{index=9}
  </p>
</section>


      <script>
        // Initialize tab functionality
        document.addEventListener('DOMContentLoaded', () => {
            const tabs = document.querySelectorAll('.tab');
            
            tabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    // Remove active class from all tabs and content
                    tabs.forEach(t => t.classList.remove('active'));
                    document.querySelectorAll('.tab-content').forEach(content => {
                        content.classList.remove('active');
                    });
                    
                    // Add active class to clicked tab and corresponding content
                    tab.classList.add('active');
                    const contentId = `${tab.dataset.tab}-content`;
                    document.getElementById(contentId).classList.add('active');
                });
            });
        });
        </script>

      

      
      <h2>Conclusion</h2>
      <p>Autoencoders are powerful tools for unsupervised learning that can be applied to a wide range of problems. From simple linear autoencoders to complex variational models, they provide a flexible framework for learning representations of data.</p>
      
      <p>As we've seen, the mathematical principles behind autoencoders are elegant and provide insights into how they work. By understanding these principles, we can better design and apply autoencoders to solve real-world problems.</p>
      
      <p>In future posts, we'll explore more advanced topics like disentangled representations, adversarial autoencoders, and applications to specific domains like natural language processing and time series analysis.</p>
    
    </div>
  </article>
  </div>  
  
  </body>
   </html>