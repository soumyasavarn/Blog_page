<!DOCTYPE html>
 <html lang="en">
  <head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width,initial-scale=1.0"> <title>Understanding Autoencoders | Soumya Savarn</title> 
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
  <script src="https://cdn.jsdelivr.net/npm/tensorflow@2.8.0/dist/tf.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
  <style> :root { --primary-color: #232526; --accent-color: #ffb347; --bg-color: #f8f9fa; --text-color: #333; --light-text: #666; --card-bg: #fff; --code-bg: #f1f1f1; }
  body {
    font-family: 'Segoe UI', Arial, sans-serif;
    margin: 0;
    background: var(--bg-color);
    color: var(--text-color);
    line-height: 1.8;
  }
  
  .back-btn {
    display: inline-block;
    padding: 8px 16px;
    color: var(--primary-color);
    text-decoration: none;
    margin-bottom: 32px;
    transition: color 0.2s;
    font-weight: 500;
    border-radius: 4px;
    background: rgba(255, 179, 71, 0.1);
  }
  
  .back-btn:hover {
    color: var(--accent-color);
    background: rgba(255, 179, 71, 0.2);
  }
  
  .blog-post {
    max-width: 800px;
    margin: 0 auto;
  }
  
  .blog-meta {
    color: var(--light-text);
    margin: 16px 0 32px;
    font-size: 0.95em;
  }
  
  .blog-content {
    line-height: 1.8;
    font-size: 1.1em;
  }
  
  .blog-content h2 {
    margin-top: 48px;
    color: var(--primary-color);
    border-bottom: 2px solid var(--accent-color);
    padding-bottom: 8px;
    display: inline-block;
  }
  
  .blog-content h3 {
    margin-top: 32px;
    color: var(--primary-color);
  }
  
  .blog-content p {
    margin-bottom: 24px;
  }
  
  .blog-content img {
    max-width: 100%;
    border-radius: 8px;
    margin: 24px 0;
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
  }
  
  .code-block {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 8px;
    overflow-x: auto;
    margin: 24px 0;
    font-family: 'Consolas', 'Monaco', monospace;
    font-size: 0.9em;
  }
  
  .visualization-container {
    margin: 40px 0;
    padding: 24px;
    background: var(--card-bg);
    border-radius: 12px;
    box-shadow: 0 4px 16px rgba(0,0,0,0.08);
  }
  
  .visualization-controls {
    display: flex;
    gap: 16px;
    margin-bottom: 24px;
    flex-wrap: wrap;
  }
  
  .control-group {
    display: flex;
    flex-direction: column;
    gap: 8px;
  }
  
  button, select {
    padding: 8px 16px;
    background: var(--primary-color);
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background 0.2s;
  }
  
  button:hover {
    background: #414345;
  }
  
  .highlight {
    background: rgba(255, 179, 71, 0.2);
    padding: 2px 4px;
    border-radius: 4px;
  }
  
  .math-formula {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 8px;
    margin: 24px 0;
    overflow-x: auto;
    text-align: center;
  }
  
  .tabs {
    display: flex;
    gap: 4px;
    margin-bottom: 24px;
    border-bottom: 2px solid #eee;
    padding-bottom: 2px;
}

.tab {
    padding: 8px 16px;
    background: #f8f9fa;
    border-radius: 4px 4px 0 0;
    cursor: pointer;
    transition: all 0.2s;
    border: 1px solid #eee;
    border-bottom: none;
}

.tab:hover {
    background: #e9ecef;
}

.tab.active {
    background: var(--primary-color);
    color: white;
    border-color: var(--primary-color);
}

.tab-content {
    display: none;
    padding: 20px;
    background: #fff;
    border-radius: 4px;
    box-shadow: 0 2px 8px rgba(0,0,0,0.05);
}
.notebook-reference {
    background: #f8f9fa;
    border-left: 4px solid #0366d6;
    padding: 16px;
    margin: 24px 0;
    border-radius: 4px;
}

.implementation-note {
    background: #f0f7ff;
    border-left: 4px solid #79b8ff;
    padding: 12px 16px;
    margin: 16px 0;
    border-radius: 4px;
    font-size: 0.95em;
}

.implementation-note h3 {
    margin-top: 0;
    color: #0366d6;
}
.tab-content.active {
    display: block;
    animation: fadeIn 0.3s ease-in;
}

@keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

.code-block {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 4px;
    margin: 16px 0;
    font-family: 'Consolas', monospace;
}

.math-formula {
    padding: 24px;
    background: var(--code-bg);
    border-radius: 4px;
    margin: 16px 0;
    overflow-x: auto;
    text-align: center;
}
  
  .progress-container {
    height: 20px;
    width: 100%;
    background-color: #e9ecef;
    border-radius: 4px;
    margin: 16px 0;
  }
  
  .progress-bar {
    height: 100%;
    width: 0;
    background-color: var(--accent-color);
    border-radius: 4px;
    transition: width 0.3s ease;
  }
  
  .tooltip {
    position: absolute;
    padding: 8px;
    background: rgba(0,0,0,0.8);
    color: white;
    border-radius: 4px;
    pointer-events: none;
    font-size: 0.9em;
    z-index: 10;
  }
  
  @media (max-width: 768px) {
    #main {
      padding: 24px 5vw !important;
    }
    
    .visualization-controls {
      flex-direction: column;
    }
  }

  .comparison-container {
    margin: 32px 0;
    background: var(--card-bg);
    padding: 24px;
    border-radius: 12px;
    box-shadow: 0 4px 16px rgba(0,0,0,0.08);
}

.comparison-table {
    width: 100%;
    border-collapse: separate;
    border-spacing: 0;
    margin-top: 20px;
    font-size: 0.95em;
}

.comparison-table th,
.comparison-table td {
    padding: 16px;
    text-align: left;
    border: 1px solid #eee;
}

.comparison-table th {
    background: var(--primary-color);
    color: white;
    font-weight: 500;
    position: relative;
}

.comparison-table th:first-child {
    border-top-left-radius: 8px;
}

.comparison-table th:last-child {
    border-top-right-radius: 8px;
}

.comparison-table tr:last-child td:first-child {
    border-bottom-left-radius: 8px;
}

.comparison-table tr:last-child td:last-child {
    border-bottom-right-radius: 8px;
}

.comparison-table td {
    background: white;
    transition: background-color 0.2s;
}

.comparison-table tr:nth-child(even) td {
    background: #f8f9fa;
}

.comparison-table tr:hover td {
    background: #f0f0f0;
}

.comparison-table td:first-child {
    font-weight: 500;
    color: var(--primary-color);
}

/* Add responsive styles */
@media (max-width: 768px) {
    .comparison-table {
        font-size: 0.85em;
    }
    
    .comparison-table th,
    .comparison-table td {
        padding: 12px;
    }
}

.comparison-grid {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 20px;
    margin: 24px 0;
}

.grid-item {
    text-align: center;
}

.architecture-img {
    max-width: 100%;
    border-radius: 8px;
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
}

.img-caption {
    margin-top: 12px;
    color: var(--light-text);
    font-size: 0.9em;
}

@media (max-width: 768px) {
    .comparison-grid {
        grid-template-columns: 1fr;
    }
}

.reference-list {
    list-style-type: none;
    padding-left: 0;
}

.reference-list li {
    margin-bottom: 12px;
    line-height: 1.6;
}

.reference-list ul {
    padding-left: 20px;
    margin-top: 8px;
}

#references h3 {
    margin-top: 24px;
    color: var(--primary-color);
    font-size: 1.2em;
}
  </style> </head> <body> <div id="main" style="padding: 32px 15vw;"> <a href="index.html" class="back-btn">&larr; Back to Home</a>
  <article class="blog-post">
    <h1>Everything about Autoencoders: From Linear Algebra to its Multimodal and Cross-modal Generative Capabilities</h1>
    <div class="blog-meta">May 5, 2025 -  Multimodal Unsupervised Learning -  By Soumya Savarn</div>
    
    <div class="blog-content">

      <!-- Motivation -->
      <section id="motivation">
        <h2>Motivation</h2>
        <p>My journey into generative models began with what seemed like an overwhelming field. However, my academic coursework in probability theory, Deep Learning, and Multimodal Data Processing provided the perfect foundation. Through this cumulative knowledge, I discovered that autoencoders offered an accessible entry point into understanding and working with Generative Multimodal models. I tried to build every concept from scratch, and you can watch this video for deeper insights into my research journey. It combines all my learning into a single comprehensive overview. Thanks to my Multimodal Data Processing course project for rekindling my passion for sharing knowledge online!</p>
      </section>

      <!-- Historical Perspective -->
      <section id="history">
        <h2>Historical Perspective</h2>
        <p>Autoencoders trace back to early neural network research in the 1980s. Variants like denoising and variational autoencoders emerged after 2010, enabling applications in image and multimodal learning. Recent advances in cross-modal generation build on these foundations.</p>
      </section>

      <h2>Introduction</h2>
      <p>Autoencoders are a type of artificial neural network used for unsupervised learning. Their primary goal is to learn efficient representations of input data, typically for dimensionality reduction or noise removal. The architecture consists of an <b>encoder</b> that compresses the input, a <b>bottleneck</b> that stores the compressed knowledge, and a <b>decoder</b> that reconstructs the original data from this compressed form. <div class="notebook-reference">
        <p>All code implementations discussed in this blog can be found in this <a href="https://www.kaggle.com/code/soumyasavarn/everything-about-autoencoders/notebook?scriptVersionId=238355572" target="_blank">Kaggle notebook</a>.</p>
      </div></p>
      
      <div class="visualization-container">
        <img src="images/diagram-auto.png" alt="General Autoencoder Architecture" class="architecture-img">
        <p class="img-caption">Basic architecture of an autoencoder showing encoder, bottleneck, and decoder components</p>
      </div>

      <p>In this blog, we'll explore autoencoders from their mathematical foundations to advanced applications in multimodal learning. We'll also implement interactive visualizations to help you understand how autoencoders work.</p>
      
      <h2>Autoencoder vs U-Net: Understanding the Differences</h2>
      <div class="comparison-container">
        <p>While both architectures use encoder-decoder structures, they serve different purposes and have key differences:</p>
        
        <table class="comparison-table">
          <tr>
            <th>Feature</th>
            <th>Autoencoder</th>
            <th>U-Net</th>
          </tr>
          <tr>
            <td>Primary Purpose</td>
            <td>Unsupervised learning, dimensionality reduction, feature learning</td>
            <td>Supervised segmentation, dense prediction tasks</td>
          </tr>
          <tr>
            <td>Architecture</td>
            <td>Symmetric encoder-decoder with bottleneck</td>
            <td>Encoder-decoder with skip connections between corresponding layers</td>
          </tr>
          <tr>
            <td>Information Flow</td>
            <td>All information passes through bottleneck</td>
            <td>Features can bypass bottleneck through skip connections</td>
          </tr>
          <tr>
            <td>Output Size</td>
            <td>Same as input (reconstruction)</td>
            <td>Can be different from input (segmentation map)</td>
          </tr>
          <tr>
            <td>Training</td>
            <td>Self-supervised (input = target)</td>
            <td>Supervised (requires labeled data)</td>
          </tr>
        </table>
      </div>
      
      <div class="visualization-container">
        <h3>Autoencoder Architecture</h3>
        <img src="images/autoencoder-architecture.png" alt="Autoencoder Architecture showing encoder, bottleneck, and decoder" class="architecture-img">
        <p class="img-caption">Basic architecture of an autoencoder showing the encoding and decoding process</p>
      </div>
      
      <h2>Mathematical Foundation</h2>
      <p>An autoencoder is defined by two main components: an <span class="highlight">encoder function</span> that transforms the input data, and a <span class="highlight">decoder function</span> that recreates the input data from the encoded representation.</p>
      
      <p>Formally, for input space $$X$$ and encoded space $$Z$$, we define:</p>
      <div class="math-formula">
        \[ \text{Encoder}: f_\phi: \mathcal{X} \rightarrow \mathcal{Z} \]
        \[ \text{Decoder}: g_\theta: \mathcal{Z} \rightarrow \mathcal{X} \]
      </div>
      
      <p>The training objective is to minimize the reconstruction error:</p>
      <div class="math-formula">
        \[ \mathcal{L}(\phi, \theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\phi(\mathbf{x}_i))\|^2 \]
      </div>
      
      <p>Surprisingly, for linear autoencoders (using only linear activations), the optimal solution is equivalent to Principal Component Analysis (PCA). This mathematical connection is well-explained in <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/src/Lecture7/modules/Module2/Lecture7_2.pdf" target="_blank">these lecture notes from IIT Madras's Deep Learning course</a> and rigorously proven in <a href="https://arxiv.org/abs/1804.10253" target="_blank">this paper by Plaut (2018)</a>. The linear autoencoder learns to project data onto the principal subspace, just like PCA.</p>
      <div class="implementation-note">
        <p>💡 For practical implementation of linear autoencoders and their equivalence to PCA, check <a href="https://www.kaggle.com/code/soumyasavarn/everything-about-autoencoders/notebook?scriptVersionId=238355572#Appendix-1:-Linear-Autoencoders-and-PCA" target="_blank">Appendix 1 of the companion notebook</a>.</p>
      </div>
      
      <div class="visualization-container comparison-grid">
        <div class="grid-item">
            <img src="images/linear_auto.png" alt="Linear Autoencoder Architecture" class="architecture-img">
            <p class="img-caption">Linear Autoencoder Structure</p>
        </div>
        <div class="grid-item">
            <img src="images/pca.png" alt="PCA Visualization" class="architecture-img">
            <p class="img-caption">Principal Component Analysis (PCA)</p>
        </div>
      </div>

      <div class="math-formula">
        \[ \mathcal{L}(\phi, \theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\phi(\mathbf{x}_i))\|^2 \]
        <p class="formula-caption">Linear autoencoder loss function, which under optimal conditions yields PCA solution</p>
      </div>
      
      <div class="visualization-container">
        <h2><strong>BONUS: </strong>Recirculation Algorithm</h2>
        <h4>You may skip this part.</h4>
  <p>
    It is very rare to find something related to it in mainstream machine learning community.
    The Recirculation Algorithm is an alternative to backpropagation for training neural networks, particularly autoencoders. It was introduced by Geoffrey Hinton and James McClelland in 1987 as a more biologically plausible learning mechanism. 
    (<a href="https://www.mdpi.com/2227-7390/11/8/1777?utm_source=chatgpt.com" target="_blank">MDPI</a>)
  </p>

  <h3> Key Reference</h3>
  <ul>
    <li>
      <strong>Hinton, G.E., & McClelland, J.L. (1987).</strong> 
      <em>Learning Representations by Recirculation</em>. 
      In <em>Proceedings of the Neural Information Processing Systems</em>, Denver, CO, USA. MIT Press: Cambridge, MA, USA.
      <br>
      <a href="https://www.mdpi.com/2227-7390/11/8/1777" target="_blank">Link to reference</a>
    </li>
  </ul>

  <p>
    In this foundational paper, Hinton and McClelland propose the recirculation algorithm as a method for training neural networks without the need for explicit error backpropagation. Instead, the network adjusts its weights based on the difference between the input and the reconstructed output, allowing for local learning rules that are more aligned with biological neural processes.
  </p>

  <h3> Overview of the Recirculation Algorithm</h3>
  <p>The recirculation algorithm operates in two main phases:</p>
  <ol>
    <li><strong>Forward Phase</strong>: The input data is passed through the network to produce an output.</li>
    <li><strong>Backward (Recirculation) Phase</strong>: The output is then fed back into the network as input, and the network processes this "recirculated" data to produce a reconstruction.</li>
  </ol>

  <p>
    The weights are updated based on the difference between the original input and the reconstruction, using local learning rules. This approach allows the network to learn representations by minimizing the discrepancy between the input and its reconstruction without relying on global error signals.
    (<a href="https://pubmed.ncbi.nlm.nih.gov/30317133/?utm_source=chatgpt.com" target="_blank">PubMed</a>)
  </p>

  <h3> Further Reading</h3>
  <ul>
    <li>
      <strong>Buscema, P.M.</strong> 
      <em>Recirculation Neural Networks</em>. 
      <a href="https://www.academia.edu/5743527/Recirculation_Neural_Networks" target="_blank">Link to paper</a>
    </li>
    <li>
      <strong>Baldi, P., & Sadowski, P. (2018).</strong> 
      <em>Learning in the Machine: Recirculation is Random Backpropagation</em>. 
      <a href="https://pubmed.ncbi.nlm.nih.gov/30317133/" target="_blank">Link to article</a>
    </li>
  </ul>

    <p>
      These works delve into the theoretical underpinnings of the recirculation algorithm and its relationship to other learning methods, offering valuable insights into alternative approaches to training neural networks.
    </p>

       
        <div class="math-formula">
          <h3>Forward Pass:</h3>
          \[ h = \sigma(W_1 x) \]
          \[ y = \sigma(W_2 h) \]
          <p class="formula-caption">where \(\sigma\) is the sigmoid activation function</p>
        </div>

        <div class="math-formula">
          <h3>Recirculation:</h3>
          \[ h_{rec} = \sigma(W_2^T y) \]
          <p class="formula-caption">where \(W_2^T\) is the transpose of the decoder weights</p>
        </div>

        <div class="math-formula">
          <h3>Weight Update:</h3>
          \[ \Delta W_1 = \eta (h_{rec} - h) x^T \]
          <p class="formula-caption">where \(\eta\) is the learning rate</p>
        </div>

        <h3>Key Properties:</h3>
        <ul>
          <li><strong>Local Learning:</strong> Updates only use information available at each synapse</li>
          <li><strong>Biological Plausibility:</strong> No separate error backpropagation channel needed</li>
          <li><strong>Hardware Efficiency:</strong> Well-suited for neuromorphic computing systems</li>
        </ul>

       
      
    </div>



      <h2>Types of Autoencoders</h2>
      <p>There are several variations of autoencoders, each with specific applications:</p>
      
      <div class="tabs">
        <div class="tab active" data-tab="vanilla">Vanilla</div>
        <div class="tab" data-tab="sparse">Sparse</div>
        <div class="tab" data-tab="denoising">Denoising</div>
        <div class="tab" data-tab="variational">Variational (VAE)</div>
        <div class="tab" data-tab="convolutional">Convolutional</div>
      </div>
      
      <div class="tab-content active" id="vanilla-content">
        <h3>Vanilla Autoencoder</h3>
        <p>The basic autoencoder architecture consists of an encoder and decoder with fully connected layers. The encoder compresses the input to a lower-dimensional code, and the decoder reconstructs the input from this code.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\theta(\mathbf{x}_i))\|^2 \]
          <p class="formula-caption">where \(f_\theta\) is the encoder and \(g_\theta\) is the decoder</p>
        </div>
        <div class="animation-container">
            <h3>Network Architecture Animation</h3>
            <canvas id="standardAutoencoder" width="600" height="200"></canvas>
            <div class="animation-controls">
                <button onclick="toggleAnimation()">Play/Pause</button>
                <button onclick="resetAnimation()">Reset</button>
            </div>
        </div>
      </div>
      
      <div class="tab-content" id="sparse-content">
        <h3>Sparse Autoencoder</h3>
        <p>Sparse autoencoders add a sparsity constraint to the hidden layer, forcing the model to activate only a small number of neurons at a time. This encourages the model to learn more robust features.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\theta(\mathbf{x}_i))\|^2 + \lambda \sum_{j=1}^m |\rho - \hat{\rho_j}| \]
          <p class="formula-caption">where \(\rho\) is the target sparsity and \(\hat{\rho_j}\) is the average activation of hidden unit j</p>
        </div>
      </div>
      
      <div class="tab-content" id="denoising-content">
        <h3>Denoising Autoencoder</h3>
        <p>Denoising autoencoders are trained to reconstruct clean inputs from corrupted versions. This makes them robust to noise and helps them learn more useful features.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\theta(\tilde{\mathbf{x}}_i))\|^2 \]
          <p class="formula-caption">where \(\tilde{\mathbf{x}}_i = \mathbf{x}_i + \epsilon\) is the corrupted input with noise \(\epsilon\)</p>
        </div>
      </div>
      
      <div class="tab-content" id="variational-content">
        <h3>Variational Autoencoder (VAE)</h3>
        <p>Variational autoencoders are probabilistic models that learn a latent variable model for the input data. Instead of encoding an input as a single point, they encode it as a distribution over the latent space.</p>
        <p>The VAE loss function has two components: the reconstruction loss and the KL divergence between the encoder's distribution and a prior distribution.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\phi, \theta, \mathbf{x}) = -\mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})] + D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})) \]
        </div>
        <div class="animation-container">
            <h3>VAE Architecture Animation</h3>
            <canvas id="vaeFlow" width="600" height="200"></canvas>
            <div class="animation-controls">
                <button onclick="toggleAnimation()">Play/Pause</button>
                <button onclick="resetAnimation()">Reset</button>
            </div>
        </div>
      </div>
      
      <div class="tab-content" id="convolutional-content">
        <h3>Convolutional Autoencoder</h3>
        <p>Convolutional autoencoders use convolutional layers in both the encoder and decoder, making them well-suited for image data.</p>
        <div class="math-formula">
          \[ f_\theta(\mathbf{x}) = \sigma(\text{Conv2D}(\mathbf{x}) \ast \mathbf{W} + \mathbf{b}) \]
          \[ g_\theta(\mathbf{z}) = \sigma(\text{Conv2DTranspose}(\mathbf{z}) \ast \mathbf{W}' + \mathbf{b}') \]
          <p class="formula-caption">where \(W, W'\) are learnable filters and \(\sigma\) is an activation function</p>
        </div>
      </div>

      <p>For a comprehensive theoretical background on Autoencoders and Variational Autoencoders (VAEs), I highly recommend Chapter 14 and 20 of <a href="https://www.deeplearningbook.org/" target="_blank">Deep Learning</a> by Goodfellow, Bengio, and Courville.</p>

      <h2>Applications</h2>
      <p>Autoencoders serve multiple purposes in machine learning and data science. They excel at dimensionality reduction by compressing high-dimensional data while preserving key features. Their ability to learn normal data patterns makes them effective for anomaly detection through reconstruction error analysis. In image processing, denoising autoencoders can clean corrupted images. The encoded representations provide valuable features for downstream tasks like classification. Additionally, variants like variational autoencoders enable generative modeling, creating new data samples that mirror the training distribution.</p>

      <p>Let's explore <strong>Variational Autoencoders</strong> (VAEs) and their fascinating applications in multimodal learning. For a rigorous mathematical treatment and detailed proofs, I highly recommend referring to <a href="https://arxiv.org/pdf/1606.05908" target="_blank">this seminal paper</a> on multimodal VAEs.</p>

       <!-- Convex Interpolation in VAEs -->
<section id="convex-interpolation">
  <h2>Convex Interpolation in Variational Autoencoders</h2>
  <p>
    I’ve found that (obviously the researchers) in a VAE, because we force the latent variable \(z\) to follow a simple prior like \(p(z)=\mathcal{N}(0,I)\), the resulting latent space becomes (approximately) convex—any linear mix 
    \[
      \mathbf{z}_\alpha = (1-\alpha)\mathbf{z}_1 + \alpha\mathbf{z}_2,\quad \alpha\in[0,1]
    \]
    stays in a high–density region where the decoder can produce meaningful outputs .
  </p>
  <p>
    Concretely, decoding \(\mathbf{z}_\alpha\) via \(\mathbf{x}_\alpha = g_\theta(\mathbf{z}_\alpha)\) often yields smooth “in‑between” samples that semantically blend the endpoints .  
  </p>
  <p>
    However, naive latent interpolations can stray off the true data manifold, leading to artifacts. Recent work shapes the latent manifold to be locally convex—e.g., by adding a regularizer that encourages
    \[
      \|g_\theta((1-\alpha)\mathbf{z}_i + \alpha \mathbf{z}_j) - ((1-\alpha)\mathbf{x}_i + \alpha \mathbf{x}_j)\|\;\text{to be small}
    \]
    for many \(\alpha\) .  
  </p>
  <p>
    Statisticians call evaluation inside the convex hull of training points “interpolation,” which is generally safer than extrapolating outside that hull .
  </p>
  <section id="thought-experiment" class="implementation-note">
    <h3>Food for Thought: Voice Interpolation</h3>
    <p>
      Think about this—what if we could map the voices of two famous singers, like Arijit Singh and Atif Aslam, and then find a smooth path from one voice to the other? Kind of like moving a slider from one style to the next. That would be amazing, right?
    </p>
    
    <p>
      Now take it a step further—what if you could see how close your voice is to your favorite singer's? Just like with face filters where you can see how your face compares to someone else's, we could do something similar with voice. You could actually learn how your voice is different, and maybe even change it to sound more like theirs.
    </p>
    
    <p>
      The idea sounds fun—and the possibilities are endless!
    </p>
  </section>
  <div class="implementation-note">
    <p>💡 To see convex interpolation in action with implementation details, visit <a href="https://www.kaggle.com/code/soumyasavarn/everything-about-autoencoders/notebook?scriptVersionId=238355572#Appendix-2:-Convex-Interpolation" target="_blank">Appendix 2 of the companion notebook</a>.</p>
  </div>

  <div class="visualization-container">
    <img src="images/convex.png" alt="Convex Interpolation in Latent Space" class="architecture-img">
    <p class="img-caption">Visualization of convex interpolation in the VAE latent space</p>
  </div>

</section>

<!-- Insights from “Multimodal Generative Models for Scalable Weakly‑Supervised Learning” -->
<section id="mvae-motivation">
  <h2>Moving to multimodal generation</h2>
  <p>
    By now you must have a good clue that the VAE is a generative model.
    It can generate new data points by sampling from the latent space and decoding them back to the original space. 
    This property is particularly useful in multimodal learning, where we want 
    to generate data that combines information from multiple modalities.
    A naive way to do this is to concatenate the latent variables from each modality and train a single VAE on the combined data.
    However, this approach has several limitations:
  </p>
  <ul>
    <li>It requires a large amount of labeled data for each modality, which is often not available.</li>
    <li>It does not leverage the relationships between modalities, leading to suboptimal representations.</li>
    <li>It can be computationally expensive and slow to train.</li>
  </ul>
</section>

<!-- Insights from “Multimodal Generative Models for Scalable Weakly‑Supervised Learning” -->
<div>
  <h2>Insights from "Multimodal Generative Models for Scalable Weakly‑Supervised Learning"</h2>

  <p>I first encountered the MVAE paper by Mike Wu and Noah Goodman (<a href="https://arxiv.org/pdf/1802.05335" target="_blank">Wu & Goodman, 2018</a>) when I was looking for a way to merge different data types—like images, text, and audio—into one shared hidden representation called <em>z</em>. In their approach, each modality's encoder outputs a Gaussian "expert" over <em>z</em>, and they multiply those experts together along with a simple Gaussian prior. This trick, known as the product‑of‑experts (PoE), means that as I feed in more modalities, the combined Gaussian becomes tighter (its variance shrinks), so I become more confident about the value of <em>z</em> when I have richer information.</p>

  <p>Behind the scenes, training the MVAE means maximizing a modified evidence lower bound (ELBO). On one hand, I want to reconstruct each modality from <em>z</em>, which pushes up the sum of log‑likelihoods for every modality. On the other hand, I add a KL divergence term that keeps my fused posterior close to the standard normal prior, preventing the model from becoming overconfident. To handle missing data, the authors alternate between fully observed examples—where all encoders and the PoE get updated—and partially observed examples—where only the single‑modality encoder for the available data is updated. This sub‑sampling trick makes sure I can still infer a sensible <em>z</em> even if some streams drop out.</p>

  <p>An elegant aspect of MVAE is its shared decoder. Instead of building one large decoder per modality, they attach small “heads” for each output type to a single backbone network. This reduces the total number of parameters and forces the model to learn a core latent space that works across all data types.</p>


  <style>
    code { background-color: #f4f4f4; padding: 2px 4px; border-radius: 4px; }
    .example { background-color: #eef; padding: 10px; border-left: 4px solid #88f; margin: 10px 0; }
  </style>

  <p>A Variational Autoencoder (VAE) models data <em>x</em> using a latent variable <em>z</em>, with a prior <code>p(z)</code> (often a standard Gaussian) and a decoder <code>p<sub>θ</sub>(x|z)</code> implemented by a neural network. Because the true data likelihood is hard to compute, we optimize the <strong>Evidence Lower Bound (ELBO)</strong> instead:</p>
  <pre><code>ELBO(x) = E<sub>q<sub>φ</sub>(z|x)</sub>[log p<sub>θ</sub>(x|z)] - KL(q<sub>φ</sub>(z|x) || p(z))</code></pre>
  <p>Here, <code>q<sub>φ</sub>(z|x)</code> is an encoder (inference network) approximating the posterior, and <code>KL</code> is the Kullback-Leibler divergence.</p>

  <h3>Extending to Multiple Modalities</h3>
  <p>If we have <em>N</em> different inputs (modalities) <code>x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>N</sub></code> (e.g., image, text, audio), we assume they are independent given <code>z</code>. The generative model becomes:</p>
  <pre><code>p(z, x<sub>1</sub>, ..., x<sub>N</sub>) = p(z) &times; ∏ p<sub>θ</sub>(x<sub>i</sub>|z)</code></pre>
  <p>When some modalities are missing, we simply ignore them in training.</p>

  <h3>Combining Encoders with Product-of-Experts</h3>
  <p>We need a combined encoder <code>q(z|x<sub>1</sub>,...,x<sub>N</sub>)</code>. A neat trick is the <em>Product-of-Experts (PoE)</em>:</p>
  <pre><code>q(z|x<sub>1</sub>,...,x<sub>N</sub>) ∝ p(z) &times; ∏ q̃(z|x<sub>i</sub>),</code></pre>
  <p>where each <code>q̃(z|x<sub>i</sub>)</code> is a Gaussian from the encoder for modality <em>i</em>, and <code>p(z)</code> acts as a "prior expert."</p>

  <h3>Numerical Example</h3>
  <div class="example">
    <p>Suppose we have <strong>2 modalities</strong> with Gaussian encoders:</p>
    <ul>
      <li>Modality 1: <code>q̃(z|x<sub>1</sub>) = N(z; μ<sub>1</sub>=2, σ<sub>1</sub><sup>2</sup>=1)</code></li>
      <li>Modality 2: <code>q̃(z|x<sub>2</sub>) = N(z; μ<sub>2</sub>=0, σ<sub>2</sub><sup>2</sup>=4)</code></li>
      <li>Prior: <code>p(z) = N(z; 0, 1)</code></li>
    </ul>
    <p>Compute precision (<code>T = 1/variance</code>):</p>
    <ul>
      <li>T<sub>1</sub> = 1/1 = 1</li>
      <li>T<sub>2</sub> = 1/4 = 0.25</li>
      <li>T<sub>0</sub> (prior) = 1/1 = 1</li>
    </ul>
    <p><strong>Combined precision:</strong> T = T<sub>0</sub> + T<sub>1</sub> + T<sub>2</sub> = 1 + 1 + 0.25 = 2.25</p>
    <p><strong>Combined mean:</strong></p>
    <pre><code>μ = (T<sub>0</sub>·0 + T<sub>1</sub>·2 + T<sub>2</sub>·0) / T = (0 + 2 + 0) / 2.25 ≈ 0.89</code></pre>
    <p><strong>Combined variance:</strong> σ<sup>2</sup> = 1 / T = 1 / 2.25 ≈ 0.44</p>
    <p>So the multimodal posterior is <code>N(z; μ≈0.89, σ≈0.66)</code>. This single Gaussian summarizes both inputs and the prior!</p>
  </div>

  <h3>Putting It All Together</h3>
  <p>1. <strong>Encode each modality</strong>: get its <code>μ<sub>i</sub></code> and <code>σ<sub>i</sub></code>.<br>
  2. <strong>Combine with PoE</strong>: use the formulas above to get joint <code>μ</code> and <code>σ</code>.<br>
  3. <strong>Reparameterize</strong>: sample <code>z = μ + σ ⊙ ε</code> with ε∼N(0, I).<br>
  4. <strong>Decode</strong>: reconstruct each modality with <code>p<sub>θ</sub>(x<sub>i</sub>|z)</code>.<br>
  5. <strong>Optimize ELBO</strong>: sum reconstruction losses and KL term.</p>

  <p>This simple approach lets us handle any combination of missing modalities without training separate encoders for each subset!</p>
  <div class="implementation-note">
    <p>💡 For a complete implementation of the Product of Experts (PoE) approach, check out <a href="https://www.kaggle.com/code/soumyasavarn/everything-about-autoencoders/notebook?scriptVersionId=238355572#Appendix-3:-Product-of-Experts" target="_blank">Appendix 3 of the companion notebook</a>.</p>
  </div>

  <div class="visualization-container">
    <img src="images/poes.png" alt="Product of Experts Architecture" class="architecture-img">
    <p class="img-caption">Product of Experts (PoE) combining multiple modality encoders</p>
  </div>
  <div class="animation-container">
      <h3>Multimodal VAE Architecture Animation</h3>
      <canvas id="multimodalFlow" width="600" height="300"></canvas>
      <div class="animation-controls">
          <button onclick="toggleAnimation()">Play/Pause</button>
          <button onclick="resetAnimation()">Reset</button>
      </div>
  </div>
</div>

<section id="references">
  <h2>References</h2>
  
  <h3>Papers</h3>
  <ul class="reference-list">
    <li>Wu, M., & Goodman, N. (2018). <a href="https://arxiv.org/pdf/1802.05335" target="_blank">Multimodal Generative Models for Scalable Weakly-Supervised Learning</a>. arXiv preprint arXiv:1802.05335.</li>
    <li>Dai, B., et al. (2021). <a href="https://arxiv.org/abs/2110.04121" target="_blank">Multimodal Conditional Image Synthesis with Product-of-Experts GANs</a>. arXiv preprint arXiv:2110.04121.</li>
    <li>Plaut, E. (2018). <a href="https://arxiv.org/abs/1804.10253" target="_blank">From Principal Subspaces to Principal Components with Linear Autoencoders</a>. arXiv preprint arXiv:1804.10253.</li>
    <li>Hinton, G.E., & McClelland, J.L. (1987). Learning Representations by Recirculation. In <em>Neural Information Processing Systems</em>.</li>
  </ul>

  <h3>Books</h3>
  <ul class="reference-list">
    <li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <a href="https://www.deeplearningbook.org/" target="_blank">Deep Learning</a>. MIT Press.</li>
  </ul>

  <h3>Videos</h3>
  <ul class="reference-list">
    <li>Deepia. (2023). <a href="https://www.youtube.com/watch?v=qJeaCHQ1k2w" target="_blank">Understanding Autoencoders: A Visual Guide</a>.</li>
  </ul>

  <h3>Tools Used</h3>
  <ul class="reference-list">
    <li><strong>Code Assistance:</strong>
      <ul>
        <li>Claude 3.7 Sonnet - JavaScript interactivity and animations</li>
        <li>GitHub Copilot - Image placement and layout assistance</li>
        <li>Microsoft OneNote - For explanation in the video</li>
      </ul>
    </li>
    <li><strong>Deployment:</strong> Vercel</li>
  </ul>
</section>

      <script>
        // Initialize tab functionality
        document.addEventListener('DOMContentLoaded', () => {
            const tabs = document.querySelectorAll('.tab');
            
            tabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    // Remove active class from all tabs and content
                    tabs.forEach(t => t.classList.remove('active'));
                    document.querySelectorAll('.tab-content').forEach(content => {
                        content.classList.remove('active');
                    });
                    
                    // Add active class to clicked tab and corresponding content
                    tab.classList.add('active');
                    const contentId = `${tab.dataset.tab}-content`;
                    document.getElementById(contentId).classList.add('active');
                });
            });
        });
        </script>

<script>
// Add canvas elements first in the HTML sections
function addCanvasElements() {
    // Add to vanilla autoencoder section
    document.querySelector('#vanilla-content').insertAdjacentHTML('beforeend', `
        <div class="animation-container">
            <h3>Network Architecture Animation</h3>
            <canvas id="standardAutoencoder" width="600" height="200"></canvas>
            <div class="animation-controls">
                <button onclick="toggleAnimation()">Play/Pause</button>
                <button onclick="resetAnimation()">Reset</button>
            </div>
        </div>
    `);

    // Add to VAE section
    document.querySelector('#variational-content').insertAdjacentHTML('beforeend', `
        <div class="animation-container">
            <h3>VAE Architecture Animation</h3>
            <canvas id="vaeFlow" width="600" height="200"></canvas>
            <div class="animation-controls">
                <button onclick="toggleAnimation()">Play/Pause</button>
                <button onclick="resetAnimation()">Reset</button>
            </div>
        </div>
    `);

    // Add to MVAE section after the PoE discussion
    document.querySelector('#mvae-motivation').insertAdjacentHTML('beforeend', `
        <div class="animation-container">
            <h3>Multimodal VAE Architecture Animation</h3>
            <canvas id="multimodalFlow" width="600" height="300"></canvas>
            <div class="animation-controls">
                <button onclick="toggleAnimation()">Play/Pause</button>
                <button onclick="resetAnimation()">Reset</button>
            </div>
        </div>
    `);
}

// Modified initialization code
document.addEventListener('DOMContentLoaded', () => {
    // Add canvas elements first
    addCanvasElements();
    
    // Initialize variables
    window.animationFrame = null;
    window.isAnimating = true;
    window.t = 0;
    
    // Get canvas contexts
    const standardCtx = document.getElementById('standardAutoencoder').getContext('2d');
    const vaeCtx = document.getElementById('vaeFlow').getContext('2d');
    const multimodalCtx = document.getElementById('multimodalFlow').getContext('2d');
    
    // Initial draw
    drawStandardAutoencoder(standardCtx);
    drawVAE(vaeCtx);
    drawMultimodalFlow(multimodalCtx);
    
    // Start animation
    animate();
    
    // Add tab functionality
    const tabs = document.querySelectorAll('.tab');
    tabs.forEach(tab => {
        tab.addEventListener('click', () => {
            tabs.forEach(t => t.classList.remove('active'));
            document.querySelectorAll('.tab-content').forEach(content => {
                content.classList.remove('active');
            });
            tab.classList.add('active');
            document.getElementById(`${tab.dataset.tab}-content`).classList.add('active');
        });
    });
});
</script>

     
    </div>
  </article>
  </div>  
  
  </body>
   </html>